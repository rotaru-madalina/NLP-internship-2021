{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "03_LanguageModels_With_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjercan/asr-toolkit/blob/master/examples/03_LanguageModels_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\"\"\"\r\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\r\n",
        "\r\n",
        "Instructions for setting up Colab are as follows:\r\n",
        "1. Open a new Python 3 notebook.\r\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\r\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\r\n",
        "4. Run this cell to set up dependencies.\r\n",
        "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\r\n",
        "\"\"\"\r\n",
        "# If you're using Google Colab and not running locally, run this cell.\r\n",
        "\r\n",
        "## Install dependencies\r\n",
        "!pip install wget\r\n",
        "!apt-get install sox libsndfile1 ffmpeg\r\n",
        "!pip install unidecode\r\n",
        "!pip install matplotlib>=3.3.2\r\n",
        "!apt-get install libsox-fmt-all libsox-dev sox > /dev/null\r\n",
        "!pip install torchaudio\r\n",
        "!python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null\r\n",
        "!pip install wandb\r\n",
        "\r\n",
        "## Install NeMo\r\n",
        "BRANCH = 'main'\r\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\r\n",
        "\r\n",
        "# install beam search decoder\r\n",
        "!apt-get install -y swig\r\n",
        "!git clone https://github.com/NVIDIA/NeMo -b \"$BRANCH\"\r\n",
        "!cd NeMo && bash scripts/asr_language_modeling/ngram_lm/install_beamsearch_decoders.sh\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\r\n",
        "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\r\n",
        "that you want to use the \"Run All Cells\" (or similar) option.\r\n",
        "\"\"\"\r\n",
        "# exit()\r\n",
        "from IPython.display import clear_output\r\n",
        "clear_output()"
      ],
      "outputs": [],
      "metadata": {
        "id": "tJ0JnCOVdMHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import wget\r\n",
        "import gzip\r\n",
        "import shutil\r\n",
        "\r\n",
        "import nemo\r\n",
        "import nemo.collections.asr as nemo_asr\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import numpy as np\r\n",
        "import augment\r\n",
        "import torchaudio\r\n",
        "import torchaudio.datasets\r\n",
        "\r\n",
        "from datetime import datetime as dt\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from asr.metrics import ASRMetricFunction, CTCLossFunction\r\n",
        "from asr.visualisation import play_audio, print_err_html, print_stats, plot_waveform\r\n",
        "from asr.general import set_parameter_requires_grad, load_checkpoint, save_checkpoint, tensors_to_device, tensor_to_string\r\n",
        "from asr.utils import ChainRunner\r\n",
        "from asr.models import GreedyDecoder, BeamSearchDecoderWithLM\r\n",
        "from asr.datasets import librispeech_dataloader\r\n",
        "from IPython.display import YouTubeVideo\r\n",
        "\r\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\r\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "MODEL_NAME='stt_en_jasper10x5dr'\r\n",
        "LM_3GRAM_PATH = '3-gram.arpa'\r\n",
        "LM_4GRAM_PATH = '4-gram.arpa'\r\n",
        "ROOT = os.path.join(\".\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2021-08-11 13:04:35 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
            "################################################################################\n",
            "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
            "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
            "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
            "################################################################################\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2021-08-11 13:04:47 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKJ-_FqFdXNo",
        "outputId": "07a6d6d0-8ca4-4e64-c417-52b3f2eef79b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def download_lm(lm_path):\n",
        "    %rm -v \"{lm_path}\"*\n",
        "    !wget \"https://www.openslr.org/resources/11/{lm_path}.gz\" -O \"{lm_path}.gz\"\n",
        "    !gzip -cdv \"{lm_path}.gz\" > \"{lm_path}\"\n",
        "\n",
        "model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=MODEL_NAME, strict=False).to(DEVICE)\n",
        "\n",
        "VOCABULARY = list(map(lambda x: x.upper(), model.decoder.vocabulary))\n",
        "vocab = VOCABULARY + ['<pad>']\n",
        "BLANK = len(vocab) - 1 \n",
        "\n",
        "DICTIONARY = dict(zip(vocab, range(len(vocab))))\n",
        "LABELS = {v:k for k, v in DICTIONARY.items()}\n",
        "\n",
        "_, test_dataloader = librispeech_dataloader(DICTIONARY, root=ROOT, urls=[\"test-clean\"], folder_in_archive=\"LibriSpeech\", batch_size=4, download=True)\n",
        "\n",
        "download_lm(LM_3GRAM_PATH)\n",
        "download_lm(LM_4GRAM_PATH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2021-08-11 13:05:29 cloud:56] Found existing object /root/.cache/torch/NeMo/NeMo_1.2.0/stt_en_jasper10x5dr/856ae08d5c4bd78b5e27f696e96f7aab/stt_en_jasper10x5dr.nemo.\n",
            "[NeMo I 2021-08-11 13:05:29 cloud:62] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.2.0/stt_en_jasper10x5dr/856ae08d5c4bd78b5e27f696e96f7aab/stt_en_jasper10x5dr.nemo\n",
            "[NeMo I 2021-08-11 13:05:29 common:681] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2021-08-11 13:06:04 modelPT:131] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /data2/voices/train_1k.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - ' '\n",
            "    - a\n",
            "    - b\n",
            "    - c\n",
            "    - d\n",
            "    - e\n",
            "    - f\n",
            "    - g\n",
            "    - h\n",
            "    - i\n",
            "    - j\n",
            "    - k\n",
            "    - l\n",
            "    - m\n",
            "    - 'n'\n",
            "    - o\n",
            "    - p\n",
            "    - q\n",
            "    - r\n",
            "    - s\n",
            "    - t\n",
            "    - u\n",
            "    - v\n",
            "    - w\n",
            "    - x\n",
            "    - 'y'\n",
            "    - z\n",
            "    - ''''\n",
            "    batch_size: 32\n",
            "    trim_silence: true\n",
            "    max_duration: 16.7\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    \n",
            "[NeMo W 2021-08-11 13:06:04 modelPT:138] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /data2/voices/train_1k_samp.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - ' '\n",
            "    - a\n",
            "    - b\n",
            "    - c\n",
            "    - d\n",
            "    - e\n",
            "    - f\n",
            "    - g\n",
            "    - h\n",
            "    - i\n",
            "    - j\n",
            "    - k\n",
            "    - l\n",
            "    - m\n",
            "    - 'n'\n",
            "    - o\n",
            "    - p\n",
            "    - q\n",
            "    - r\n",
            "    - s\n",
            "    - t\n",
            "    - u\n",
            "    - v\n",
            "    - w\n",
            "    - x\n",
            "    - 'y'\n",
            "    - z\n",
            "    - ''''\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2021-08-11 13:06:04 features:252] PADDING: 16\n",
            "[NeMo I 2021-08-11 13:06:04 features:269] STFT using torch\n",
            "[NeMo I 2021-08-11 13:06:29 save_restore_connector:143] Model EncDecCTCModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.2.0/stt_en_jasper10x5dr/856ae08d5c4bd78b5e27f696e96f7aab/stt_en_jasper10x5dr.nemo.\n",
            "removed '3-gram.arpa'\n",
            "removed '3-gram.arpa.gz'\n",
            "--2021-08-11 13:06:30--  https://www.openslr.org/resources/11/3-gram.arpa.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 759636181 (724M) [application/x-gzip]\n",
            "Saving to: ‘3-gram.arpa.gz’\n",
            "\n",
            "3-gram.arpa.gz      100%[===================>] 724.45M   109MB/s    in 7.0s    \n",
            "\n",
            "2021-08-11 13:06:37 (104 MB/s) - ‘3-gram.arpa.gz’ saved [759636181/759636181]\n",
            "\n",
            "3-gram.arpa.gz:\t 68.3%\n",
            "removed '4-gram.arpa'\n",
            "removed '4-gram.arpa.gz'\n",
            "--2021-08-11 13:07:22--  https://www.openslr.org/resources/11/4-gram.arpa.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1355172078 (1.3G) [application/x-gzip]\n",
            "Saving to: ‘4-gram.arpa.gz’\n",
            "\n",
            "4-gram.arpa.gz      100%[===================>]   1.26G  29.6MB/s    in 30s     \n",
            "\n",
            "2021-08-11 13:07:52 (43.6 MB/s) - ‘4-gram.arpa.gz’ saved [1355172078/1355172078]\n",
            "\n",
            "4-gram.arpa.gz:\t 69.2%\n"
          ]
        }
      ],
      "metadata": {
        "id": "UIYddQ4idps3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c944d9-9055-496c-d6b2-408d443934f4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Test model\n",
        "greedy_lm = GreedyDecoder(LABELS, BLANK)\n",
        "\n",
        "model.eval()\n",
        "metric_fn = ASRMetricFunction()\n",
        "loss_fn = CTCLossFunction(blank=BLANK)\n",
        "loop = tqdm(test_dataloader, position=0, leave=True)\n",
        "\n",
        "for batch_idx, tensors in enumerate(loop):\n",
        "    valid_lengths, waveform, target_lengths, utterance = tensors_to_device(tensors, DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        loss_fn(log_probs.permute(1, 0, 2), utterance, encoded_len, target_lengths)\n",
        "\n",
        "        transcriptions = greedy_lm(greedy_predictions, predictions_len=encoded_len)\n",
        "\n",
        "    metric_fn(tensor_to_string(utterance, target_lengths, LABELS), transcriptions)\n",
        "\n",
        "    loop.set_postfix(loss=loss_fn.show())\n",
        "loop.close()\n",
        "print(metric_fn.show())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 655/655 [18:51<00:00,  1.73s/it, loss=(ctc:0.0531)]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======ASRModel========\n",
            "WER=4.1016\tCER=1.2654\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faua8y2KeOwY",
        "outputId": "514fd075-afd2-48a2-bcd6-7ea314137ddb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Test model\n",
        "print(\"Testing without languange model\")\n",
        "beam_search_lm = BeamSearchDecoderWithLM(\n",
        "    vocab=VOCABULARY,\n",
        "    beam_width=16,\n",
        "    alpha=1.5, beta=1.5,\n",
        "    lm_path=None,\n",
        "    num_cpus=max(os.cpu_count(), 1))\n",
        "\n",
        "def best_transcriptions(transcriptions):\n",
        "    return list(map(lambda xs: xs[0][1], transcriptions))\n",
        "\n",
        "model.eval()\n",
        "metric_fn = ASRMetricFunction()\n",
        "loss_fn = CTCLossFunction(blank=BLANK)\n",
        "loop = tqdm(test_dataloader, position=0, leave=True)\n",
        "\n",
        "for batch_idx, tensors in enumerate(loop):\n",
        "    valid_lengths, waveform, target_lengths, utterance = tensors_to_device(tensors, DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        loss_fn(log_probs.permute(1, 0, 2), utterance, encoded_len, target_lengths)\n",
        "\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    metric_fn(tensor_to_string(utterance, target_lengths, LABELS), best_transcriptions(transcriptions))\n",
        "\n",
        "    loop.set_postfix(loss=loss_fn.show())\n",
        "loop.close()\n",
        "print(metric_fn.show())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing without languange model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 655/655 [19:42<00:00,  1.81s/it, loss=(ctc:0.0531)]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======ASRModel========\n",
            "WER=4.1086\tCER=1.2684\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6G1g5hFRHKS",
        "outputId": "4d04faf9-2bc6-4bc0-94dc-9b5f7924ff4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Test model\n",
        "print(\"Testing 3-gram languange model\")\n",
        "beam_search_lm = BeamSearchDecoderWithLM(\n",
        "    vocab=VOCABULARY,\n",
        "    beam_width=16,\n",
        "    alpha=1.5, beta=1.5,\n",
        "    lm_path=LM_3GRAM_PATH,\n",
        "    num_cpus=max(os.cpu_count(), 1))\n",
        "\n",
        "def best_transcriptions(transcriptions):\n",
        "    return list(map(lambda xs: xs[0][1], transcriptions))\n",
        "\n",
        "model.eval()\n",
        "metric_fn = ASRMetricFunction()\n",
        "loss_fn = CTCLossFunction(blank=BLANK)\n",
        "loop = tqdm(test_dataloader, position=0, leave=True)\n",
        "\n",
        "for batch_idx, tensors in enumerate(loop):\n",
        "    valid_lengths, waveform, target_lengths, utterance = tensors_to_device(tensors, DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        loss_fn(log_probs.permute(1, 0, 2), utterance, encoded_len, target_lengths)\n",
        "\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    metric_fn(tensor_to_string(utterance, target_lengths, LABELS), best_transcriptions(transcriptions))\n",
        "\n",
        "    loop.set_postfix(loss=loss_fn.show())\n",
        "loop.close()\n",
        "print(metric_fn.show())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 3-gram languange model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/655 [00:00<?, ?it/s][NeMo W 2021-08-11 13:11:08 patch_utils:50] torch.stft() signature has been updated for PyTorch 1.7+\n",
            "    Please update PyTorch to remain compatible with later versions of NeMo.\n",
            "[NeMo W 2021-08-11 13:11:08 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "      return torch.floor_divide(self, other)\n",
            "    \n",
            "100%|██████████| 655/655 [19:11<00:00,  1.76s/it, loss=(ctc:0.0531)]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======ASRModel========\n",
            "WER=3.7385\tCER=1.3473\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhI8NTD0Afd9",
        "outputId": "9cfdf544-9c19-4090-d8dc-17a72d508fb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Test model\n",
        "print(\"Testing 4-gram languange model\")\n",
        "beam_search_lm = BeamSearchDecoderWithLM(\n",
        "    vocab=VOCABULARY,\n",
        "    beam_width=16,\n",
        "    alpha=1.5, beta=1.5,\n",
        "    lm_path=LM_4GRAM_PATH,\n",
        "    num_cpus=max(os.cpu_count(), 1))\n",
        "\n",
        "def best_transcriptions(transcriptions):\n",
        "    return list(map(lambda xs: xs[0][1], transcriptions))\n",
        "\n",
        "model.eval()\n",
        "metric_fn = ASRMetricFunction()\n",
        "loss_fn = CTCLossFunction(blank=BLANK)\n",
        "loop = tqdm(test_dataloader, position=0, leave=True)\n",
        "\n",
        "for batch_idx, tensors in enumerate(loop):\n",
        "    valid_lengths, waveform, target_lengths, utterance = tensors_to_device(tensors, DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        loss_fn(log_probs.permute(1, 0, 2), utterance, encoded_len, target_lengths)\n",
        "\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    metric_fn(tensor_to_string(utterance, target_lengths, LABELS), best_transcriptions(transcriptions))\n",
        "\n",
        "    loop.set_postfix(loss=loss_fn.show())\n",
        "loop.close()\n",
        "print(metric_fn.show())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/655 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 4-gram languange model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 655/655 [19:12<00:00,  1.76s/it, loss=(ctc:0.0531)]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======ASRModel========\n",
            "WER=3.7676\tCER=1.3546\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1icEVkJp7r9",
        "outputId": "b50bf608-2b17-4343-9edb-fa86caf3d0f0"
      }
    }
  ]
}